{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of Hilary Clinton's Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read data from sqlite\n",
    "\n",
    "con = sqlite3.connect('database.sqlite')\n",
    "e = pd.read_sql_query(\"select distinct id_sentiment.field2 as id, id_sentiment.field3 as sentiment, \\\n",
    "MetadataSubject || ' ' || ExtractedBodyText as extractText From id_sentiment, Emails \\\n",
    "where id_sentiment.field2=Emails.id order by id;\",con)\n",
    "\n",
    "print(len(e.extractText));\n",
    "\n",
    "cs = []\n",
    "for i in range(len(e.extractText)): #len(e.ExtractedBodyText)):\n",
    "    cs.append(str(e.extractText[i]))\n",
    "    #print('#')\n",
    "    #print(e.extractText[i])\n",
    "    \n",
    "print(len(cs))\n",
    "import pandas as pd\n",
    "a = pd.DataFrame(e)\n",
    "a.to_csv(\"new_labeled.csv\")\n",
    "#print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#text cleaning, tokenization, stemming, remove punctuation etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1=e\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONGRATULATIONS!! This is an major achievement, and an even greater disaster averted. AM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'congratulations!! this is an major achievement, and an even greater disaster averted. am'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singleStr = data1['extractText'][220]\n",
    "print(singleStr)\n",
    "singleStr = singleStr.lower()\n",
    "singleStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "from string import punctuation\n",
    "print(string.punctuation)\n",
    "stop_words = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','what','which',\n",
    "              'is','If','while','this', 'at', 'i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(data1)):\n",
    "    singleStr=data1['extractText'][i]\n",
    "    singleStr=singleStr.lower()\n",
    "    singleStr=text_to_wordlist(singleStr)\n",
    "    data1['extractText'][i]=singleStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat = data1\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = dat[2470]\n",
    "tokens = word_tokenize(text)\n",
    "# stemming of words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "#print(stemmed[:100])\n",
    "set(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(stemmed))\n",
    "set(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp=dat\n",
    "count=0\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "for i in range(2470-1):\n",
    "    text = dat[i]\n",
    "    tokens = word_tokenize(text)\n",
    "    # stemming of words\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in tokens]\n",
    "    tmp[i] = set(stemmed)\n",
    "    count=count+1\n",
    "    print(count)\n",
    "for i in range(2471,len(dat)):\n",
    "    text = dat[i]\n",
    "    tokens = word_tokenize(text)\n",
    "    # stemming of words\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in tokens]\n",
    "    tmp[i] = set(stemmed)\n",
    "    count=count+1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1 is happy, 2 is unhappy, 3 is neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read in sentiment manually labeled csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_tfidfdat2=pd.read_csv('subdat_labeled.csv')\n",
    "len(dat2),'#', \n",
    "dat2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp2=[]\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "for i in range(len(dat2)):\n",
    "    text = dat2['text'][i]\n",
    "    tokens = word_tokenize(text)\n",
    "    # stemming of words\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in tokens]\n",
    "    #tmp2[i] = set(stemmed)\n",
    "    tmp2.append(stemmed)\n",
    "    #count=count+1\n",
    "    #print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat2=dat2.replace(0,3)\n",
    "dat2.head(10)\n",
    "#dat2['sentiment'][1], dat2['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat2['text'], \n",
    "                                                    dat2['sentiment'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('X_train first entry:\\n\\n', X_train.iloc[2])\n",
    "print('\\n\\nX_train shape: ', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "len(vect.get_feature_names())\n",
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "X_train_vectorized\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "print (clf.score(X_train_vectorized, y_train))\n",
    "predictions = clf.predict(vect.transform(X_test))\n",
    "#print clf.score(validation_vecs_ugdbow_tgdmm, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "auc(np.array([1, 1, 2, 3, 3]),np.array([3, 2, 3, 3, 3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test, '#,', predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "print('AUC: ', auc(np.array(y_test), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = CountVectorizer(ngram_range=(2, 3))\n",
    "print(v.fit([\"an apple a day keeps the doctor away\"]).vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the CountVectorizer to the training data specifiying a minimum \n",
    "# document frequency of 5 and extracting 1-grams and 2-grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(min_df=6, ngram_range=(1,3)).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_vectorizer =  CountVectorizer(stop_words='english', min_df=1, ngram_range=(3,3))\n",
    "bigram_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the CountVectorizer to the training data specifiying a minimum \n",
    "# document frequency of 5 and extracting 1-grams and 2-grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer(min_df=7).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#60.8% AUC score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "ytest=np.array(y_test)\n",
    "#print('AUC: ', roc_auc_score(ytest, predictions))\n",
    "np.array(y_test), '#,', predictions\n",
    "# Compute the error\n",
    "fpr, tpr, thresholds = metrics.roc_curve(np.array(y_test), predictions, pos_label=1)\n",
    "print(\"Logistic regression AUC: {0}\".format(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(predictions), type(np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(y_test),'#',predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roc_auc_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "#vect = TfidfVectorizer(min_df=8).fit(X_train)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=15, min_df=8, max_features=no_features, stop_words='english')\n",
    "vect = tfidf_vectorizer.fit(X_train)\n",
    "len(vect.get_feature_names())\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "#print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "np.array(y_test),'#',predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jumaoyuan/anaconda/envs/APS/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "#topic modeling\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "no_features = 500\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=15, min_df=8, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(X_train)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, '#', 52)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_feature_names), '#', len(tf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "no_topics = 3\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "#\n",
      "world life want team schedule yes week foreign ll report\n",
      "Topic 1:\n",
      "#\n",
      "state did time got ve people talk way day family\n",
      "Topic 2:\n",
      "#\n",
      "thank need com fyi sid day way tomorrow pm sent\n"
     ]
    }
   ],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print('#')\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 10\n",
    "#display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#max_df is used for removing terms that appear too frequently, \n",
    "#also known as \"corpus-specific stop words\". For example:\n",
    "#max_df = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    "#max_df = 25 means \"ignore terms that appear in more than 25 documents\".\n",
    "#The default max_df is 1.0, which means \"ignore terms that appear in more than 100% of the documents\". \n",
    "#Thus, the default setting does not ignore any terms.\n",
    "\n",
    "#min_df is used for removing terms that appear too infrequently. For example:\n",
    "#min_df = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\n",
    "#min_df = 5 means \"ignore terms that appear in less than 5 documents\".\n",
    "#The default min_df is 1, which means \"ignore terms that appear in less than 1 document\". \n",
    "#Thus, the default setting does not ignore any terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#full codes\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print \"Topic %d:\" % (topic_idx)\n",
    "        print \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LDA visualization: pyLDAvis\n",
    "import graphlab as gl\n",
    "import pyLDAvis\n",
    "import pyLDAvis.graphlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print (documents[doc_index])\n",
    "\n",
    "# Single line documents from http://web.eecs.utk.edu/~berry/order/node4.html#SECTION00022000000000000000\n",
    "documents = [\n",
    "            \"Human machine interface for Lab ABC computer applications\",\n",
    "            \"A survey of user opinion of computer system response time\",\n",
    "            \"The EPS user interface management system\",\n",
    "            \"System and human system engineering testing of EPS\",\n",
    "            \"Relation of user-perceived response time to error measurement\",\n",
    "            \"The generation of random, binary, unordered trees\",\n",
    "            \"The intersection graph of paths in trees\",\n",
    "            \"Graph minors IV: Widths of trees and quasi-ordering\",\n",
    "            \"Graph minors: A survey\"\n",
    "            ]\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 2\n",
    "\n",
    "# Run NMF\n",
    "nmf_model = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "nmf_W = nmf_model.transform(tfidf)\n",
    "nmf_H = nmf_model.components_\n",
    "\n",
    "# Run LDA\n",
    "lda_model = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "lda_W = lda_model.transform(tf)\n",
    "lda_H = lda_model.components_\n",
    "\n",
    "no_top_words = 4\n",
    "no_top_documents = 4\n",
    "display_topics(nmf_H, nmf_W, tfidf_feature_names, documents, no_top_words, no_top_documents)\n",
    "display_topics(lda_H, lda_W, tf_feature_names, documents, no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ANN with Tfidf vectorizer\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_validation, y_validation = X_train, y_train, X_test, y_test\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n",
    "    t0 = time()\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    #print \"null accuracy: {0:.2f}%\".format(null_accuracy*100)\n",
    "    #print \"accuracy score: {0:.2f}%\".format(accuracy*100)\n",
    "    if accuracy > null_accuracy:\n",
    "        print (\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy == null_accuracy:\n",
    "        print (\"model has the same accuracy with the null accuracy\")\n",
    "    else:\n",
    "        print (\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    #print \"train and test time: {0:.2f}s\".format(train_test_time)\n",
    "    #print \"-\"*80\n",
    "    return accuracy, train_test_time\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "lr = LogisticRegression()\n",
    "n_features = np.arange(10000,100001,10000)\n",
    "\n",
    "def nfeature_accuracy_checker(vectorizer=cvec, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=lr):\n",
    "    result = []\n",
    "    #print (classifier)\n",
    "    #print \"\\n\"\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        #print \"Validation result for {} features\".format(n)\n",
    "        nfeature_accuracy,tt_time = accuracy_summary(checker_pipeline, x_train, y_train, x_validation, y_validation)\n",
    "        result.append((n,nfeature_accuracy,tt_time))\n",
    "    return result\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer()\n",
    "\n",
    "feature_result_ugt = nfeature_accuracy_checker(vectorizer=tvec)\n",
    "feature_result_bgt = nfeature_accuracy_checker(vectorizer=tvec,ngram_range=(1, 2))\n",
    "feature_result_tgt = nfeature_accuracy_checker(vectorizer=tvec,ngram_range=(1, 3))\n",
    "\n",
    "\n",
    "nfeatures_plot_tgt = pd.DataFrame(feature_result_tgt,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "nfeatures_plot_bgt = pd.DataFrame(feature_result_bgt,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "nfeatures_plot_ugt = pd.DataFrame(feature_result_ugt,columns=['nfeatures','validation_accuracy','train_test_time'])\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(nfeatures_plot_tgt.nfeatures, nfeatures_plot_tgt.validation_accuracy,label='trigram tfidf vectorizer',color='royalblue')\n",
    "#plt.plot(nfeatures_plot_tg.nfeatures, nfeatures_plot_tg.validation_accuracy,label='trigram count vectorizer',linestyle=':', color='royalblue')\n",
    "plt.plot(nfeatures_plot_bgt.nfeatures, nfeatures_plot_bgt.validation_accuracy,label='bigram tfidf vectorizer',color='orangered')\n",
    "#plt.plot(nfeatures_plot_bg.nfeatures, nfeatures_plot_bg.validation_accuracy,label='bigram count vectorizer',linestyle=':',color='orangered')\n",
    "plt.plot(nfeatures_plot_ugt.nfeatures, nfeatures_plot_ugt.validation_accuracy, label='unigram tfidf vectorizer',color='gold')\n",
    "#plt.plot(nfeatures_plot_ug.nfeatures, nfeatures_plot_ug.validation_accuracy, label='unigram count vectorizer',linestyle=':',color='gold')\n",
    "plt.title(\"N-gram(1~3) test result : Accuracy\")\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Validation set accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MLP for the IMDB problem\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SA with DL (sentiment analysis, deep learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "\n",
    "data = pd.read_csv(\"subdat_labeled.csv\")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000, split=' ')\n",
    "\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "X = tokenizer.texts_to_sequences(data['text'])\n",
    "X = pad_sequences(X)\n",
    "Y = data['sentiment']\n",
    "\n",
    "# We can then create our train and test sets:\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dat2['text'], \n",
    "                                                    dat2['sentiment'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data1['extractText'], \n",
    "                                                    data1['sentiment'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182, 61, (182,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test), X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://suruchifialoke.com/2017-06-10-sentiment-analysis-movie/\n",
    "#naive bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "# Generate counts from text using a vectorizer  \n",
    "# We can choose from other available vectorizers, and set many different options\n",
    "# This code performs our step of computing word counts\n",
    "\n",
    "#vectorizer = CountVectorizer(stop_words='english', max_df=.05)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#bigram\n",
    "vect =  CountVectorizer(stop_words='english', min_df=2, ngram_range=(1,1))\n",
    "vect.fit_transform(X_train)\n",
    "\n",
    "#ti-idf\n",
    "#no_features = 300\n",
    "#tfidf_vectorizer = TfidfVectorizer(min_df=2, max_features=no_features, stop_words='english')\n",
    "#vect = tfidf_vectorizer.fit(X_train)\n",
    "#len(vect.get_feature_names())\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_vectorized\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "#vect = CountVectorizer().fit(X_train)\n",
    "#len(vect.get_feature_names())\n",
    "# transform the documents in the training data to a document-term matrix\n",
    "#X_train_vectorized = vect.fit_transform(X_train)\n",
    "X_test_vectorized = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<182x905 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3266 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vectorized, [int(r) for r in y_train])\n",
    "predictions = nb.predict(X_test_vectorized)\n",
    "predictions\n",
    "actual = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 3, 3, 3, 1, 3, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1,\n",
       "       3, 1, 3, 3, 2, 1, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 1, 2, 2, 3, 3, 3,\n",
       "       2, 3, 3, 3, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual =list(map(int, actual))\n",
    "actual=np.asarray(actual)\n",
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 3, 3, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1,\n",
       "       3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 2, 1, 2, 1, 3, 3, 3,\n",
       "       2, 3, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   1  2   3\n",
       "Actual              \n",
       "1          24  2   1\n",
       "2           2  6   4\n",
       "3           4  0  18"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "y_actu = pd.Series(actual, name='Actual')\n",
    "y_pred = pd.Series(predictions, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7868852459016393"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "48/61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7868852459016393"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(actual, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomal naive bayes AUC: 0.14705882352941177\n"
     ]
    }
   ],
   "source": [
    "# Compute the error\n",
    "fpr, tpr, thresholds = metrics.roc_curve(actual, predictions, pos_label=1)\n",
    "print(\"Multinomal naive bayes AUC: {0}\".format(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#vect = CountVectorizer(min_df=6, ngram_range=(1,3)).fit(X_train)\n",
    "#X_train_vectorized = vect.transform(X_train)\n",
    "#bigram\n",
    "vect =  CountVectorizer(stop_words='english', min_df=5, ngram_range=(1,1))\n",
    "vect.fit_transform(X_train)\n",
    "\n",
    "#ti-idf\n",
    "#no_features = 300\n",
    "#tfidf_vectorizer = TfidfVectorizer(min_df=2, max_features=no_features, stop_words='english')\n",
    "#vect = tfidf_vectorizer.fit(X_train)\n",
    "#len(vect.get_feature_names())\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_vectorized\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3346"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9945054945054945\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "X_train_vectorized\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "print (clf.score(X_train_vectorized, y_train))\n",
    "predictions = clf.predict(vect.transform(X_test))\n",
    "#print clf.score(validation_vecs_ugdbow_tgdmm, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 3, 3, 3, 1, 3, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1,\n",
       "       3, 1, 3, 3, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 2, 1, 2, 2, 3, 3, 3,\n",
       "       2, 3, 3, 3, 1, 1, 3, 1, 1, 1, 1, 3, 2, 1, 1, 2, 1])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 3, 3, 3, 1, 3, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1,\n",
       "       3, 1, 3, 3, 2, 1, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 1, 2, 2, 3, 3, 3,\n",
       "       2, 3, 3, 3, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = np.array(y_test)\n",
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   1  2   3\n",
       "Actual              \n",
       "1          23  3   1\n",
       "2           1  9   2\n",
       "3           0  0  22"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "y_actu = pd.Series(actual, name='Actual')\n",
    "y_pred = pd.Series(predictions, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8688524590163934"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "53/61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8852459016393442"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(actual, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data1['extractText'], \n",
    "                                                    data1['sentiment'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jumaoyuan/anaconda/envs/APS/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<182x300 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1864 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://appliedmachinelearning.wordpress.com/2017/02/12/sentiment-analysis-using-tf-idf-weighting-pythonscikit-learn/\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "model1 = LinearSVC()\n",
    "\n",
    "\n",
    "#bigram\n",
    "#vect =  CountVectorizer(stop_words='english', min_df=1, ngram_range=(5,5))\n",
    "#vect.fit_transform(X_train)\n",
    "\n",
    "#ti-idf\n",
    "no_features = 300\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, max_features=no_features, stop_words='english')\n",
    "vect = tfidf_vectorizer.fit(X_train)\n",
    "len(vect.get_feature_names())\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_vectorized\n",
    "\n",
    "#model = LogisticRegression()\n",
    "#model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "#predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "#print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jumaoyuan/anaconda/envs/APS/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "model1.fit(X_train_vectorized, y_train)\n",
    "predictions = model1.predict(vect.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 3, 3, 3, 1, 3, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1,\n",
       "       3, 1, 3, 3, 2, 1, 3, 2, 3, 3, 2, 3, 2, 3, 2, 2, 1, 2, 2, 3, 3, 3,\n",
       "       2, 3, 3, 3, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = np.array(y_test)\n",
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 3, 3, 3, 3, 1, 3, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1,\n",
       "       3, 1, 3, 3, 2, 1, 3, 2, 3, 3, 1, 3, 2, 3, 3, 2, 1, 2, 3, 3, 3, 3,\n",
       "       1, 3, 3, 3, 1, 1, 3, 1, 1, 1, 1, 3, 2, 1, 1, 2, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9016393442622951"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(actual, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   1  2   3\n",
       "Actual              \n",
       "1          25  2   0\n",
       "2           2  8   2\n",
       "3           0  0  22"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "y_actu = pd.Series(actual, name='Actual')\n",
    "y_pred = pd.Series(predictions, name='Predicted')\n",
    "df_confusion = pd.crosstab(y_actu, y_pred)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAF5CAYAAACm4JG+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f7115f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#n-gram\n",
    "x=[1,2,3,4,5]\n",
    "y=[0.9180,0.5738,0.4426,0.3770, 0.3770]\n",
    "f1=plt.figure(figsize=(6,4))\n",
    "plt.plot(x,y, 'ro-')\n",
    "plt.axis([1,5,0,1])\n",
    "plt.xlabel('n-gram')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n",
    "f1.savefig('n-gram.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Sentiment Analysis | Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing import sequence\n",
    "#from LSTMattempt import load_data\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numpy.random.seed(7)\n",
    "top_words = 100\n",
    "\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data1['extractText'], \n",
    "                                                    data1['sentiment'], \n",
    "                                                    random_state=0)\n",
    "\n",
    "#print(\"x_train\", X_train)\n",
    "#print(\"y_train\",y_train)\n",
    "#pdb.set_trace()\n",
    "#exit(0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_fatures = 4\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data1['extractText'].values)\n",
    "X = tokenizer.texts_to_sequences(data1['extractText'].values)\n",
    "X = pad_sequences(X)\n",
    "print(len(X))\n",
    "\n",
    "Y = pd.get_dummies(data1['sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_vecor_length = 200\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=X_train.shape[1]))\n",
    "#model.add(LSTM(100))\n",
    "#model.add(Flatten())\n",
    "model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "    \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax')) #sigmoid\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, epochs=10, batch_size=3)\n",
    "score= model.evaluate(X_test, Y_test, verbose = 0, batch_size = batch_size)\n",
    "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras\n",
    "X_train_tfidfdat2=pd.read_csv('subdat_labeled.csv')\n",
    "data = data[['text', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1584,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1584-6a919190f3f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[^a-zA-z0-9\\s]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'sentiment'"
     ]
    }
   ],
   "source": [
    "data = data[data.sentiment != 0]\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "print(data[ data['sentiment'] == 1].size)\n",
    "print(data[ data['sentiment'] == 2].size)\n",
    "\n",
    "for idx,row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')\n",
    "    \n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1583,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jumaoyuan/anaconda/envs/APS/lib/python3.5/site-packages/ipykernel/__main__.py:12: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "/Users/jumaoyuan/anaconda/envs/APS/lib/python3.5/site-packages/ipykernel/__main__.py:13: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_77 (Embedding)     (None, 98, 128)           512       \n",
      "_________________________________________________________________\n",
      "lstm_45 (LSTM)               (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 255,706\n",
      "Trainable params: 255,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1], dropout=0.2,))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(data['sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, nb_epoch = 7, batch_size=batch_size, verbose = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#validation_size = 100\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
    "for x in range(len(X_validate)):\n",
    "    \n",
    "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "   \n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            neg_correct += 1\n",
    "        else:\n",
    "            pos_correct += 1\n",
    "       \n",
    "    if np.argmax(Y_validate[x]) == 0:\n",
    "        neg_cnt += 1\n",
    "    else:\n",
    "        pos_cnt += 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n",
    "print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSTM with word2vec embeddings\n",
    "#https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings\n",
    "\n",
    "#NLP and SA with Python\n",
    "#http://pythonforengineers.com/natural-language-processing-and-sentiment-analysis-with-python/\n",
    "\n",
    "#visualizing graphlab's LDA topicmodel with pyLDAvis\n",
    "#http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/GraphLab.ipynb\n",
    "\n",
    "#Text with LSTM Recurrent NN in Py with Keras\n",
    "#https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SA with DL\n",
    "#http://thelillysblog.com/2017/10/02/sentiment-analysis-with-deep-learning/\n",
    "\n",
    "Graph \n",
    "\n",
    "Tools\n",
    "Before we start we need to make sure we have the following tools installed:\n",
    "\n",
    "Python\n",
    "TensorFlow - Google’s open sourced numeric computational library\n",
    "Keras - Neural Network Framework, which can run on top of TensorFlow\n",
    "Numpy - Package for scientific computations\n",
    "Pandas - Package providing easy-to-use data structures and data analysis tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    " \n",
    "import json\n",
    " \n",
    "from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN, CNN, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.random.seed(7)\n",
    "top_words = 1000\n",
    "\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data1['extractText'], \n",
    "                                                    data1['sentiment'], \n",
    "                                                    random_state=0)\n",
    "\n",
    "#print(\"x_train\", X_train)\n",
    "#print(\"y_train\",y_train)\n",
    "#pdb.set_trace()\n",
    "#exit(0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1585,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "243\n",
      "(162, 98) (162, 3)\n",
      "(81, 98) (81, 3)\n"
     ]
    }
   ],
   "source": [
    "max_fatures = 4\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data1['extractText'].values)\n",
    "X = tokenizer.texts_to_sequences(data1['extractText'].values)\n",
    "X = pad_sequences(X)\n",
    "print(len(X))\n",
    "\n",
    "Y = pd.get_dummies(data1['sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1588,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jumaoyuan/anaconda/envs/APS/lib/python3.5/site-packages/ipykernel/__main__.py:10: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_80 (Embedding)     (None, 98, 128)           512       \n",
      "_________________________________________________________________\n",
      "lstm_48 (LSTM)               (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 250)               25250     \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 3)                 753       \n",
      "=================================================================\n",
      "Total params: 306,365\n",
      "Trainable params: 306,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "162/162 [==============================] - 7s 42ms/step - loss: 0.6900 - acc: 0.6235\n",
      "Epoch 2/5\n",
      "162/162 [==============================] - 2s 9ms/step - loss: 0.6738 - acc: 0.6605\n",
      "Epoch 3/5\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.6431 - acc: 0.6667\n",
      "Epoch 4/5\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.6044 - acc: 0.6667\n",
      "Epoch 5/5\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.6218 - acc: 0.6667\n"
     ]
    }
   ],
   "source": [
    "max_words = len(X_train)\n",
    "verbose=0\n",
    "batchsize=100\n",
    "layersize=250\n",
    "epoch=5\n",
    "hidden=4\n",
    "model = Sequential()\n",
    "    #print X_train\n",
    "#model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1], dropout=0.2,))\n",
    "model.add(LSTM(100))\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(layersize, activation='relu'))\n",
    "for h in range(1,hidden):\n",
    "    model.add(Dense(layersize, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "if (verbose == 1):\n",
    "    print(model.summary())\n",
    "\n",
    "#Fit the model\n",
    "#model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epoch, batch_size=batchsize, verbose=verbose)\n",
    "model.fit(X_train, Y_train, epochs=epoch, batch_size=batchsize)\n",
    "# Final evaluation of the model\n",
    "#score_test = model.evaluate(X_test, y_test, verbose=verbose)\n",
    "#score_train = model.evaluate(X_train, y_train, verbose=verbose)\n",
    "score= model.evaluate(X_test, Y_test, verbose = 0, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#score= model.evaluate(X_test, Y_test, verbose = 0, batch_size = batch_size)\n",
    "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text clearning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stop_words=True, stem_words=False):\n",
    "    # Clean the text, with the option to remove stop_words and to stem words.\n",
    "\n",
    "    # Clean the text\n",
    "    #text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"\", text)\n",
    "    text = re.sub(r\"What's\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"won't\", \"will not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"60k\", \" 60000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" America \", text)\n",
    "    text = re.sub(r\" USA \", \" America \", text)\n",
    "    text = re.sub(r\" u s \", \" America \", text)\n",
    "    text = re.sub(r\" uk \", \" England \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text)\n",
    "    text = re.sub(r\"india\", \"India\", text)\n",
    "    text = re.sub(r\"switzerland\", \"Switzerland\", text)\n",
    "    text = re.sub(r\"china\", \"China\", text)\n",
    "    text = re.sub(r\"chinese\", \"Chinese\", text) \n",
    "    text = re.sub(r\"imrovement\", \"improvement\", text)\n",
    "    text = re.sub(r\"intially\", \"initially\", text)\n",
    "    text = re.sub(r\"quora\", \"Quora\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"demonitization\", \"demonetization\", text) \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\"KMs\", \" kilometers \", text)\n",
    "    text = re.sub(r\" cs \", \" computer science \", text) \n",
    "    text = re.sub(r\" upvotes \", \" up votes \", text)\n",
    "    text = re.sub(r\" iPhone \", \" phone \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"gps\", \"GPS\", text)\n",
    "    text = re.sub(r\"gst\", \"GST\", text)\n",
    "    text = re.sub(r\"programing\", \"programming\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"dna\", \"DNA\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"the US\", \"America\", text)\n",
    "    text = re.sub(r\"Astrology\", \"astrology\", text)\n",
    "    text = re.sub(r\"Method\", \"method\", text)\n",
    "    text = re.sub(r\"Find\", \"find\", text) \n",
    "    text = re.sub(r\"banglore\", \"Banglore\", text)\n",
    "    text = re.sub(r\" J K \", \" JK \", text)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stop_words:\n",
    "        text = text.split()\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [APS]",
   "language": "python",
   "name": "Python [APS]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
